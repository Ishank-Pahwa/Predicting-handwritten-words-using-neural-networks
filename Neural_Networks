#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Nov  6 19:06:05 2019

@author: ishank
"""

#%%
import csv
import numpy as np
import matplotlib.pyplot as plt
from random import randrange
#%%
data=[]
with open("data.csv") as csv_file:
    csv_reader = csv.reader(csv_file)
    for row in csv_reader:
        data.append(row)
    table=np.array(data)
print(table.shape)
#%%
label=(table[:,784])
label=label.astype(np.float)
print(label.shape)
design=table[:,0:784]
design=design.astype(np.float)
print(design.shape)
#%%
design = (design - design.min())/(design.max() - design.min())
#%% one hot encoding
def enc(label):
    encode=np.zeros((len(label),10))
    for i in range(0,len(label)):
        temp=int(label[i])
        encode[i,temp]=1
    return encode
#%%
encode=enc(label)
#%%
x_train=design[0:int(3000*0.75),:]
x_test=design[int(3000*0.75):3000,:]
y_train=label[0:int(3000*0.75)]
y_test=label[int(3000*0.75):3000]
#%%
y_train_encode=enc(y_train)
y_test_encode=enc(y_test)
#%%
class Layer:
    def __init__(self, n_input, n_neurons, activation=None, weights=None, bias=None):
 
        self.weights = weights if weights is not None else np.random.randn(n_input, n_neurons)
        self.activation = activation
        self.bias = bias if bias is not None else np.random.randn(n_neurons)
        self.last_activation = None
        self.error = None
        self.delta = None
 
    def activate(self, x):
        r = np.dot(x, self.weights) + self.bias
        self.last_activation = self._apply_activation(r)
        return self.last_activation
 
    def _apply_activation(self, r):
        if self.activation is None:
            return r
        if self.activation == 'tanh':
            return np.tanh(r)
        if self.activation == 'sigmoid':
            return 1.0 / (1.0 + np.exp(-r))
        if self.activation=='softmax':
            return np.exp(r) / np.sum(np.exp(r), axis = 1, keepdims = True)
        return r
 
    def apply_activation_derivative(self, r):
        if self.activation is None:
            return r
 
        if self.activation == 'tanh':
            return 1.0 - r ** 2.0
 
        if self.activation == 'sigmoid':
            return r * (1.0 - r)
        return r
 
class NeuralNetwork:
    def __init__(self):
        self._layers = []
 
    def add_layer(self, layer):
        self._layers.append(layer)
 
    def feed_forward(self, X):
        for layer in self._layers:
            X = layer.activate(X)
 
        return X
    def wt(self):
        return np.dot(self._layers[0].weights,self._layers[1].weights)
    def predict(self, X):
        ff = self.feed_forward(X)
        return np.argmax(ff, axis=1)
 
    def backpropagation(self, X, y, learning_rate, lamb):
        output = self.feed_forward(X)
        for i in reversed(range(len(self._layers))):
            layer = self._layers[i]
            if layer == self._layers[-1]:
                layer.delta = output - y
            else:
                next_layer = self._layers[i + 1]
                layer.error = np.dot(next_layer.delta,next_layer.weights.T) 
                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)

        for i in range(len(self._layers)):
            layer = self._layers[i]
            input_to_use = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)
            layer.weights = (1 - learning_rate * lamb) * layer.weights - learning_rate * np.dot(input_to_use.T ,layer.delta)
            layer.bias-=np.sum(layer.delta,axis=0)*learning_rate
 
    def train(self, X, y, learning_rate, max_epochs, lamb):
        accc=[]
        x_axis=[]
        test_acc=[]
        for i in range(max_epochs):
            self.backpropagation(X, y, learning_rate, lamb)
            #print(type(accc))
            accc.append(self.accuracy(self.predict(X),y_train))
            test_acc.append(self.accuracy(self.predict(x_test),y_test))
            x_axis.append(i)
            if(i%10==0):
                acc=self.accuracy(self.predict(X),y_train)
                print(i,acc) 
        return accc,test_acc,x_axis
    @staticmethod
    def accuracy(y_pred, y_true):
        count=0
        for i in range(0,len(y_true)):
            if((y_pred[i] == y_true[i])):
                count=count+1
        #print(count)
        return count/len(y_true)
#%%
def cross_validation_split(X, y, folds):
    X_split = list()
    y_split = list()
    X_copy = list(X)
    y_copy = list(y)
    fold_size = int(len(X) / folds)
    for i in range(folds):
        X_fold = list()
        y_fold = list()
        while len(X_fold) < fold_size:
            index = randrange(len(X_copy))
            X_fold.append(X_copy.pop(index))
            y_fold.append(y_copy.pop(index))
        X_split.append(np.array(X_fold))
        y_split.append(np.reshape(np.array(y_fold), (fold_size,1)))
    return X_split, y_split
#%%
nn = NeuralNetwork()
nn.add_layer(Layer(784, 25, 'sigmoid'))
#nn.add_layer(Layer(500,200,'tanh'))
nn.add_layer(Layer(25, 10, 'softmax'))
#%%
train_acc,test_acc,x = nn.train(x_train, y_train_encode, 0.0007, 300,7)
predi=nn.predict(x_test)
#%%
wo=nn.wt()
#%%
print(nn.accuracy(predi,y_test))
#%%
plt.title('Accuracy graph')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
e,=plt.plot(x,train_acc,label='train error')
f,=plt.plot(x,test_acc, label='test error')
plt.legend(handles=[e,f])
#%% regularization
c_range=np.linspace(1,10,num=10)
regu=[]
for c in c_range:
    nn = NeuralNetwork()
    nn.add_layer(Layer(784, 400, 'sigmoid'))
    #nn.add_layer(Layer(500,200,'tanh'))
    nn.add_layer(Layer(400, 10, 'softmax'))
    train_acc,test_acc,x = nn.train(x_train, y_train_encode, 0.0007, 200,c)
    predi=nn.predict(x_test)
    regu.append(nn.accuracy(predi,y_test))
print(np.amax(regu))
print(np.where(np.amax(regu)==regu))
#%%
plt.title('Finding Regularization Parameter')
plt.xlabel('Lambda')
plt.ylabel('Test Accuracy')
e,=plt.plot(c_range,regu,label='training accuracy')
#f,=plt.plot(x,test_acc, label='test error')
plt.legend(handles=[e])
#%%
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras import regularizers
#%%
def accur(y_pred, y_true):
    count=0
    for i in range(0,len(y_true)):
        if((y_pred[i] == y_true[i])):
            count=count+1
    #print(count)
    return count/len(y_true)
#%%
classifier = Sequential()
classifier.add(Dense(output_dim = 400, init = 'uniform', activation = 'sigmoid', input_dim = 784,kernel_regularizer=regularizers.l2(7)))
#classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))
classifier.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax',kernel_regularizer=regularizers.l2(7)))
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
classifier.fit(x_train, y_train_encode, batch_size = 1000, nb_epoch = 100)
y_pred = classifier.predict(x_test)
y_pred=np.argmax(y_pred, axis=1)
ans=accur(y_test,y_pred)
print(ans)
#%%
c_range=np.linspace(0.0000001,0.001,num=10)
regu=[]
for c in c_range:
    classifier = Sequential()
    classifier.add(Dense(output_dim = 400, init = 'uniform', activation = 'sigmoid', input_dim = 784,kernel_regularizer=regularizers.l2(c)))
    #classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))
    classifier.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax',kernel_regularizer=regularizers.l2(c)))
    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
    classifier.fit(x_train, y_train_encode, batch_size = 1000, nb_epoch = 100)
    y_pred = classifier.predict(x_test)
    y_pred=np.argmax(y_pred, axis=1)
    ans=accur(y_test,y_pred)
    regu.append(ans)
print(np.amax(regu))
print(np.where(np.amax(regu)==regu))
#%%
plt.title('Finding Regularization Parameter')
plt.xlabel('Lambda')
plt.ylabel('Test Accuracy')
e,=plt.plot(c_range,regu,label='training accuracy')
#f,=plt.plot(x,test_acc, label='test error')
plt.legend(handles=[e])
#%%
image_index = 161
plt.imshow(x_test[image_index].reshape(28, 28).T,cmap='autumn')
#%%
wo=wo.T
#%%
img=wo[3]
#%%
img=(img - img.min())/(img.max() - img.min())
img=img*255
img=np.array(img,dtype='int')
#%%
plt.imshow(img.reshape(28,28).T,cmap='gray')
#%% pca
data=[]
with open("data pca.csv") as csv_file:
    csv_reader = csv.reader(csv_file)
    for row in csv_reader:
        data.append(row)
    table=np.array(data)
print(table.shape)
#%%
label=(table[:,25])
label=label.astype(np.float)
print(label.shape)
design=table[:,0:25]
design=design.astype(np.float)
print(design.shape)
#%%
encode=enc(label)
#%%
x_train=design[0:int(3000*0.75),:]
x_test=design[int(3000*0.75):3000,:]
y_train=label[0:int(3000*0.75)]
y_test=label[int(3000*0.75):3000]
#%%
y_train_encode=enc(y_train)
y_test_encode=enc(y_test)
#%%
nn = NeuralNetwork()
nn.add_layer(Layer(25, 20, 'sigmoid'))
#nn.add_layer(Layer(500,200,'tanh'))
nn.add_layer(Layer(20, 10, 'softmax'))
#%%
train_acc,test_acc,x = nn.train(x_train, y_train_encode, 0.0007, 500,0)
predi=nn.predict(x_test)

#%%
print(nn.accuracy(predi,y_test))
#%% extra credit
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten,MaxPooling2D,Dropout
#%%
from keras.datasets import mnist
(x_train,y_train),(x_test,y_test)=mnist.load_data()
#%%
y_train_encode=enc(y_train)
y_test_encode=enc(y_test)
#%%
x_train = x_train.reshape(60000,28,28,1)
x_test = x_test.reshape(10000,28,28,1)
#%%
c_range=np.linspace(0.1,0.5,num=5)
regu=[]
for c in c_range:
    model = Sequential()
    model.add(Conv2D(64, kernel_size=5,strides=2, activation='relu', input_shape=(28,28,1)))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Conv2D(32, kernel_size=5, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(400, activation='relu'))
    model.add((Dropout(c)))
    model.add(Dense(10, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(x_train, y_train_encode, validation_data=(x_test, y_test_encode), epochs=10)
#%%
model.fit(x_train, y_train_encode, validation_data=(x_test, y_test_encode), epochs=10)
#%%
ynew = model.predict_classes(x_test)
#%%
x_test=x_test.reshape(10000,784)
x_train=x_train.reshape(60000,784)
#%%
classifier = Sequential()
classifier.add(Dense(output_dim = 400, init = 'uniform', activation = 'sigmoid', input_dim = 784,kernel_regularizer=regularizers.l2(0)))
#classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))
classifier.add(Dense(output_dim = 10, init = 'uniform', activation = 'softmax',kernel_regularizer=regularizers.l2(0)))
classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
classifier.fit(x_train, y_train_encode, batch_size = 1000, nb_epoch = 100)
y_pred = classifier.predict(x_test)
y_pred=np.argmax(y_pred, axis=1)
ans=accur(y_test,y_pred)
print(ans)